{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import pylab\n",
    "from pykalman import KalmanFilter\n",
    "import time\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local own implementations\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from PLDS_Macke2015 import PLDS, EM, print_par\n",
    "\n",
    "def standard(x):\n",
    "    return (x-np.nanmean(x))/np.sqrt(np.nanvar(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulate data\n",
    "below I simulate from the PLDS model by creating a PLDS object\n",
    "\n",
    "the latent is a linear dynamical system:\n",
    "$$x_t=Ax_{t-1}+\\epsilon$$\n",
    "$$\\epsilon\\sim N(0, Q)$$\n",
    "$$x_0\\sim N(\\mu_o, Q_0)$$\n",
    "\n",
    "the observed is Poisson\n",
    "$$y_t\\sim Poisson\\left(exp(Cx_t+Bs_t)\\right)$$\n",
    "where B is the stimulus coefficient and s is the stimulus at time point t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate LDS latent and PLDS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise type (if False, simulate a regular Kalman LDS with Gaussian noise)\n",
    "poisson = True \n",
    "#### parameters used ####\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "# number of time points in a trial\n",
    "n_step = np.array([20])\n",
    "# number of trials\n",
    "Ttrials = 100\n",
    "# latent dimension\n",
    "xdim = 2\n",
    "# time constant of latent\n",
    "A = 0.99*np.eye(xdim) \n",
    "# latent noise parameters\n",
    "sigQ = 0.1 \n",
    "Q = np.eye(xdim)*sigQ  \n",
    "Q0 = np.eye(xdim)*sigQ\n",
    "x0 = np.zeros([xdim])+np.random.randn(2)*.1 # this is mu_0, sorry\n",
    "# observed dimension (number of neurons)\n",
    "ydim = 100\n",
    "# loadings, mapping function from latent to observed\n",
    "cscal = 2\n",
    "C = cscal*np.random.rand(ydim*xdim).reshape(ydim, xdim)\n",
    "# if observed is Gaussian and not Poisson (so if poisson=False above) we have the Gaussian noise term here:\n",
    "R = np.eye(ydim)*.01\n",
    "# stimulus dimension\n",
    "sdim = 1\n",
    "# stimulus coefficient\n",
    "B = cscal*np.ones([ydim,sdim])#*np.random.randn(ydim, sdim)\n",
    "# stimulus (if stimulus dimension is 1, there is only an offset firing rate)\n",
    "S = np.ones([n_step[0], sdim, Ttrials])\n",
    "if sdim>1:\n",
    "    S[:,0,:] = np.round(np.random.rand(n_step[0]*Ttrials)).reshape(n_step[0],Ttrials)\n",
    "S = np.array(S, dtype='int')\n",
    "# create model\n",
    "MOD = PLDS(xdim=xdim, ydim=ydim, n_step=n_step, C=C, Q0=Q0, A=A, Q=Q, x0=x0,R=R,\n",
    "        Ttrials=Ttrials, B=B)\n",
    "# sample from model\n",
    "MOD.sample(poisson=poisson, X=S)\n",
    "# visualize\n",
    "MOD.vis_xy()\n",
    "# get data (observed neuron activity)\n",
    "data = MOD.y.copy() # code expects data to be T by ydim by Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full EM fitting\n",
    "fit a PLDS model (latent and model parameters) given the sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MOD.y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson=True\n",
    "fit = EM(maxiter=50, gtol=1e-03)\n",
    "# define maximum number of iterations and when to stop looking for better solutions (ltol is amount of likelihood increase)\n",
    "maxiterem=3\n",
    "ltol=1e-1\n",
    "# estimate the model parameters via the EM algorithm\n",
    "# output is a PLDS class object with the fitted parameters\n",
    "# and Var_latent which is a list with #trials entries\n",
    "# each has itself #time points entries\n",
    "# that gives the latent covariance matrix for each time point\n",
    "# and tells you something about how much uncertainty there is about the latent\n",
    "MOD_fit, Var_latent = fit.fit(data=data, xdim=2, poisson=poisson, \n",
    "               seed=1, S=S, maxiterem=maxiterem, ltol=ltol,\n",
    "              cscal=.1, sigQ = 0.01 , a=.1, sigR=.1) # the last 4 parameters help initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_par(MOD_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOD.xdim == 2:\n",
    "    # visualize the 2-dim latent of an example trial\n",
    "    ttrial = 0\n",
    "    fig, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "    ax.plot(MOD_fit.x[:,0,ttrial], MOD_fit.x[:,1,ttrial], '-k')\n",
    "    for tt in range(MOD_fit.x.shape[0]):\n",
    "        evals = np.linalg.eigvals(Var_latent[ttrial][tt][0])\n",
    "        e = Ellipse((MOD_fit.x[tt,0,ttrial], MOD_fit.x[tt,1,ttrial]), \n",
    "               width = 2*np.sqrt(evals[0]), height= 2*np.sqrt(evals[1]),\n",
    "                angle=np.arctan2(evals[0]-Var_latent[ttrial][tt][0][0,0], Var_latent[ttrial][tt][0][0,1]))\n",
    "        ax.add_artist(e)\n",
    "        e.set_alpha(.4)\n",
    "        e.set_facecolor('k')\n",
    "    \n",
    "ax.set_xlabel('first fitted latent dimension')\n",
    "ax.set_ylabel('second fitted latent dimension')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-validation\n",
    "via the reconstruction error\n",
    "\n",
    "leave-neuron-out approach\n",
    "\n",
    "can be used to assess the dimensionality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing data:\n",
    "\n",
    "# stimulus during testing data\n",
    "S_test = np.ones([n_step[0], sdim, Ttrials])\n",
    "if sdim>1:\n",
    "    S_test[:,0,:] = np.round(np.random.rand(n_step[0]*Ttrials)).reshape(n_step[0],Ttrials)\n",
    "S_test = np.array(S, dtype='int')\n",
    "\n",
    "# create testing data\n",
    "MOD.sample(poisson=poisson, X=S_test, seed=10)\n",
    "data_test = MOD.y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a fitted model MOD_fit and test data (+stimuli at test time)\n",
    "# computing the reconstruction error can take some time, therefore there is an option of just computing it for\n",
    "# some neurons (if neurons=None, it will be computed for all neurons)\n",
    "neurons = np.array([10, 30, 50])\n",
    "pred, mse, mu = fit.reconstruction(data_test, S_test, MOD_fit, poisson=poisson, neurons=neurons)\n",
    "# output: pred is the prediction for each neuron from the model, if a neuron was not used there will be NaNs, \n",
    "# the shape is always time points by neurons by trials,\n",
    "# mse is the averaged square prediction error over time points and trials, a vector of #neurons entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "for nn in neurons:\n",
    "    plt.plot(data_test[:,nn,:].ravel(), pred[:,nn,:].ravel(), '.')\n",
    "plt.xlabel('true spikes')\n",
    "plt.ylabel('estimates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details: inference and learning\n",
    "below are stepwise tests of the different functions and implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttrial=0\n",
    "print('test for likelihood function P(y|x)')\n",
    "MOD.test_log_lik(MOD.x[:MOD.n_step[ttrial],:,ttrial], \n",
    "                MOD.B, MOD.C, Rtmp=MOD.R, poisson=poisson, X=S[:MOD.n_step[ttrial],:,ttrial])\n",
    "MOD.test_J_log_lik(MOD.x[:MOD.n_step[ttrial],:,ttrial],\n",
    "                MOD.B, MOD.C, Rtmp=MOD.R, poisson=poisson, X=S[:MOD.n_step[ttrial],:,ttrial])\n",
    "MOD.test_H_log_lik(MOD.x[:MOD.n_step[ttrial],:,ttrial], \n",
    "                MOD.B, MOD.C, Rtmp=MOD.R, poisson=poisson, X=S[:MOD.n_step[ttrial],:,ttrial])\n",
    "print(' ')\n",
    "print('test for prior function Px)')\n",
    "MOD.test_log_prior(MOD.x[:MOD.n_step[ttrial],:,ttrial], MOD.A, \n",
    "                     MOD.Q, MOD.Q0, MOD.x0)\n",
    "MOD.test_J_log_prior(MOD.x[:MOD.n_step[ttrial],:,ttrial], MOD.A, \n",
    "                     MOD.Q, MOD.Q0, MOD.x0)\n",
    "MOD.test_H_log_prior(MOD.x[:MOD.n_step[ttrial],:,ttrial], MOD.A, \n",
    "                     MOD.Q, MOD.Q0)\n",
    "print(' ')\n",
    "print('test for block-list conversion')\n",
    "MOD.test_block(np.round(MOD.H_log_posterior(MOD.x[:MOD.n_step[ttrial],:,ttrial], MOD.y[:MOD.n_step[ttrial],:,ttrial], MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "              MOD.Q0, MOD.x0, MOD.R, poisson=poisson, X=S[:MOD.n_step[ttrial],:,ttrial])), mdim=[MOD.xdim, MOD.xdim], offdiag=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtmp0 = np.sqrt(np.nanvar(MOD.x))*np.random.randn(MOD.x.shape[0], MOD.x.shape[1], MOD.x.shape[2])\n",
    "\n",
    "Xres = MOD.inference(Xtmp0, MOD.y, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "              MOD.Q0, MOD.x0, MOD.R, X=S, poisson=poisson, \n",
    "              disp=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "for ff in range(MOD.xdim):\n",
    "    ax[ff].plot(MOD.x[:,ff,:].ravel(), '-k', label='true')\n",
    "    ax[ff].plot(Xtmp0[:,ff,:].ravel(), '--', color='grey', label='initialization')\n",
    "    ax[ff].plot(Xres[:,ff,:].ravel(), '-r', label='impl')\n",
    "    ax[ff].set_xlabel('time')\n",
    "    ax[ff].set_ylabel('x')\n",
    "ax[ff].legend()\n",
    "fig.tight_layout()\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "for ff in range(MOD.xdim):\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()),\n",
    "               standard(MOD.x[:,ff,:].ravel()), '-k')\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()),\n",
    "                standard(Xtmp0[:,ff,:].ravel()),'.', color='grey')\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()), \n",
    "                standard(Xres[:,ff,:].ravel()),'r.')\n",
    "    ax[ff].set_xlabel('standardized true x')\n",
    "    ax[ff].set_ylabel('standardized estimated x')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if poisson: \n",
    "    R = np.eye(MOD.ydim)*np.sqrt(np.nanvar(MOD.y))\n",
    "    y = MOD.y.copy()\n",
    "    y[y<=0] = 1e-10\n",
    "    y = np.log(y)\n",
    "else: \n",
    "    R = MOD.R.copy()\n",
    "    y = MOD.y.copy()\n",
    "Xkal = np.zeros(MOD.x.shape)*np.nan\n",
    "for ttrial in range(MOD.Ttrials):\n",
    "    kf = KalmanFilter(n_dim_state=MOD.xdim, n_dim_obs=MOD.ydim,\n",
    "                     transition_matrices=MOD.A,\n",
    "                     transition_covariance=MOD.Q,\n",
    "                     observation_matrices=MOD.C,\n",
    "                     observation_covariance=R, \n",
    "                     initial_state_mean=MOD.x0,\n",
    "                     initial_state_covariance=MOD.Q0)\n",
    "    # Kalman filtering\n",
    "    filtered_state_means, filtered_state_covariances = kf.filter(y[:,:,ttrial])\n",
    "    # Kalman smoothing\n",
    "    smoothed_state_means, smoothed_state_covariances = kf.smooth(y[:,:,ttrial])\n",
    "    Xkal[:,:,ttrial] = smoothed_state_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "for ff in range(MOD.xdim):\n",
    "    ax[ff].plot(MOD.x[:,ff,:].ravel(), '-k')\n",
    "    ax[ff].plot(Xtmp0[:,ff,:].ravel(), '--', color='grey', label='initialization')\n",
    "    ax[ff].plot(Xres[:,ff,:].ravel(), '-r', label='impl')\n",
    "    ax[ff].plot(Xkal[:,ff,:].ravel(), '-g', label='kf')\n",
    "    ax[ff].set_xlabel('time')\n",
    "    ax[ff].set_ylabel('x')\n",
    "ax[ff].legend()\n",
    "fig.tight_layout()\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "for ff in range(MOD.xdim):\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()),\n",
    "               standard(MOD.x[:,ff,:].ravel()), '-k')\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()),\n",
    "                standard(Xtmp0[:,ff,:].ravel()),'.', color='grey')\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()), \n",
    "                standard(Xres[:,ff,:].ravel()),'r.')\n",
    "    ax[ff].plot(standard(MOD.x[:,ff,:].ravel()), \n",
    "                standard(Xkal[:,ff,:].ravel()),'g.')\n",
    "    ax[ff].set_xlabel('standardized true x')\n",
    "    ax[ff].set_ylabel('standardized estimated x')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if y was noiseless (= rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytmp = np.zeros(MOD.y.shape)\n",
    "for kk in range(MOD.Ttrials):\n",
    "    if poisson:\n",
    "        ytmp[:,:,kk] = np.exp(MOD.C.dot(MOD.x[:,:,kk].T)+MOD.B.dot(S[:,:,kk].T)).T\n",
    "    else:\n",
    "        ytmp[:,:,kk] = (MOD.C.dot(MOD.x[:,:,kk].T)+MOD.B.dot(S[:,:,kk].T)).T\n",
    "\n",
    "\n",
    "X0 = np.sqrt(np.nanvar(MOD.x))*np.random.randn(MOD.x.shape[0], MOD.x.shape[1], MOD.x.shape[2])\n",
    "\n",
    "mu = MOD.inference(X0, ytmp, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "              MOD.Q0, MOD.x0, MOD.R, X=S, poisson=poisson, \n",
    "              disp=False)\n",
    "\n",
    "#mu, sigma = MOD.E_step(MOD.x,ytmp, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "#                      MOD.Q0, MOD.x0, MOD.R, X=S,\n",
    "#                       poisson=poisson, disp=False)\n",
    "fig, ax = plt.subplots(1,3,figsize=(10,3))\n",
    "ax[0].plot(MOD.x[:,0,:].ravel(),MOD.x[:,1,:].ravel(), 'ok')\n",
    "ax[0].plot(mu[:,0,:].ravel(),mu[:,1,:].ravel(), 'b')\n",
    "ax[0].set_xlabel('x dim 1')\n",
    "ax[0].set_xlabel('x dim 2')\n",
    "ax[1].plot(MOD.x[:,0,:].ravel(), mu[:,0,:].ravel(), '.')  \n",
    "ax[2].plot(MOD.x[:,1,:].ravel(), mu[:,1,:].ravel(), '.')  \n",
    "for aa in range(2):\n",
    "    ax[aa].set_xlabel('true x')\n",
    "    ax[aa].set_ylabel('fitted x')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,3))\n",
    "ax[0].plot(mu[:,0,:].ravel(), 'b')\n",
    "ax[0].plot(MOD.x[:,0,:].ravel(), 'k')\n",
    "ax[1].plot(mu[:,1,:].ravel(), 'b')\n",
    "ax[1].plot(MOD.x[:,1,:].ravel(), 'k')\n",
    "for aa in range(2):\n",
    "    ax[aa].set_xlabel('time')\n",
    "    ax[aa].set_ylabel('x')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noiseless (xtmp, ytmp) versus with noise MOD.y\n",
    "mu, sigma = MOD.E_step(MOD.x,MOD.y, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "                      MOD.Q0, MOD.x0, MOD.R, X=S,\n",
    "                       poisson=poisson, disp=False)\n",
    "if poisson: \n",
    "    R = np.eye(MOD.ydim)*np.sqrt(np.nanvar(MOD.y))\n",
    "    y = MOD.y.copy()\n",
    "    y[y<=0] = 1e-10\n",
    "    y = np.log(y)\n",
    "else: \n",
    "    R = MOD.R.copy()\n",
    "    y = MOD.y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KalmanFilter(n_dim_state=MOD.xdim, n_dim_obs=MOD.ydim,\n",
    "                 transition_matrices=MOD.A,\n",
    "                 transition_covariance=MOD.Q,\n",
    "                 observation_matrices=MOD.C,\n",
    "                 observation_covariance=R, \n",
    "                 #initial_state_mean=MOD.x0,\n",
    "                 initial_state_covariance=MOD.Q0\n",
    "                 )\n",
    "kf.em(y[:,:,0], n_iter=20)\n",
    "print('x0 estimated via Kalman: \\n', kf.initial_state_mean)\n",
    "print('x0 estimated via PLDS-impl: \\n', np.round(MOD.upx0(mu),4))\n",
    "print('true x0: \\n', MOD.x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KalmanFilter(n_dim_state=MOD.xdim, n_dim_obs=MOD.ydim,\n",
    "                 transition_matrices=MOD.A,\n",
    "                 transition_covariance=MOD.Q,\n",
    "                 observation_matrices=MOD.C,\n",
    "                 observation_covariance=R, \n",
    "                 initial_state_mean=MOD.x0,\n",
    "                 #initial_state_covariance=MOD.Q0\n",
    "                 )\n",
    "kf.em(y[:,:,0], n_iter=20)\n",
    "print('Q0 estimated via Kalman: \\n', kf.initial_state_covariance)\n",
    "print('Q0 estimated via PLDS-impl: \\n', np.round(MOD.upQ0(MOD.x0, mu, [sigma[kk][0][0] for kk in range(MOD.Ttrials)]),4))\n",
    "print('true Q0: \\n', MOD.Q0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if poisson: R = np.eye(MOD.ydim)*np.sqrt(np.nanvar(MOD.y))\n",
    "else: R = MOD.R.copy()\n",
    "kf = KalmanFilter(n_dim_state=MOD.xdim, n_dim_obs=MOD.ydim,\n",
    "                 transition_matrices=MOD.A,\n",
    "                 #transition_covariance=MOD.Q,\n",
    "                 observation_matrices=MOD.C,\n",
    "                 observation_covariance=R, \n",
    "                 initial_state_mean=MOD.x0,\n",
    "                 initial_state_covariance=MOD.Q0)\n",
    "kf.em(y[:,:,0], n_iter=20)\n",
    "print('Q estimated via Kalman: \\n', kf.transition_covariance)\n",
    "print('Q estimated via PLDS-impl: \\n', np.round(MOD.upQ(MOD.A, mu, sigma),6))\n",
    "print('true Q: \\n', MOD.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if poisson: R = np.eye(MOD.ydim)*np.sqrt(np.nanvar(MOD.y))\n",
    "else: R = MOD.R.copy()\n",
    "kf = KalmanFilter(n_dim_state=MOD.xdim, n_dim_obs=MOD.ydim,\n",
    "                 #transition_matrices=MOD.A,\n",
    "                 transition_covariance=MOD.Q,\n",
    "                 observation_matrices=MOD.C,\n",
    "                 observation_covariance=R, \n",
    "                 initial_state_mean=MOD.x0,\n",
    "                 initial_state_covariance=MOD.Q0)\n",
    "kf.em(y[:,:,0], n_iter=20)\n",
    "print('A estimated via Kalman: \\n', kf.transition_matrices)\n",
    "print('A estimated via PLDS-impl: \\n', np.round(MOD.upA(mu, sigma),3))\n",
    "print('true A: \\n', MOD.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters of observation space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noiseless (xtmp, ytmp) versus with noise\n",
    "mu, sigma = MOD.E_step(MOD.x,MOD.y, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "                      MOD.Q0, MOD.x0, MOD.R, X=S,\n",
    "                       poisson=poisson, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('L(x) for true x: ', -MOD.L_dyn(MOD.x, mu, sigma, MOD.x0, MOD.Q0))\n",
    "print('L(x) for random x: ', -MOD.L_dyn(np.random.randn(MOD.x.shape[0], MOD.x.shape[1], MOD.x.shape[2]),\n",
    "          mu, sigma, MOD.x0, MOD.Q0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B0 = np.random.randn(MOD.B.shape[0], MOD.B.shape[1])\n",
    "Best = MOD.upB(B0, MOD.C, MOD.y, mu, sigma, X=S, Rtmp=MOD.R,\n",
    "             disp=True, gtol=1e-02, maxiter=10, poisson=poisson)\n",
    "fig, ax = plt.subplots(1,np.max([2,MOD.B.shape[1]]),figsize=(8,3))\n",
    "for bb in range(MOD.B.shape[1]):\n",
    "    ax[bb].plot(MOD.B[:,bb], MOD.B[:,bb], '-k')\n",
    "    ax[bb].plot(MOD.B[:,bb], B0[:,bb], '.', color='grey')\n",
    "    ax[bb].plot(MOD.B[:,bb], Best[:,bb], '.r')\n",
    "    ax[bb].set_title('stimulus dimension '+np.str(bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if poisson==False:\n",
    "    print('Warning: numerical approx of C with Gaussian noise not yet working')\n",
    "C0 = np.random.randn(MOD.ydim, MOD.xdim)\n",
    "Cest = MOD.upC(C0, MOD.B, MOD.y, mu, sigma, X=S, Rtmp=MOD.R,\n",
    "        disp=False, gtol=1e-5, maxiter=50, poisson=poisson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(8,3))\n",
    "for xx in range(2):\n",
    "    ax[xx].plot(MOD.C[:,xx], C0[:,xx], '.', color='grey', label='$C_{start}$')\n",
    "    ax[xx].plot(MOD.C[:,xx], Cest[:,xx], '.r', label='$C_{final}$')\n",
    "    ax[xx].set_title('C for latent dimension '+np.str(bb+1))\n",
    "    ax[xx].set_xlabel('true C')\n",
    "    ax[xx].set_ylabel('estimated C')\n",
    "ax[0].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test: gradient of neg lower bound given noiseless data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytmp = np.zeros(MOD.y.shape)\n",
    "for kk in range(MOD.Ttrials):\n",
    "    if poisson:\n",
    "        ytmp[:,:,kk] = np.exp(MOD.C.dot(MOD.x[:,:,kk].T)+MOD.B.dot(S[:,:,kk].T)).T\n",
    "    else:\n",
    "        ytmp[:,:,kk] = (MOD.C.dot(MOD.x[:,:,kk].T)+MOD.B.dot(S[:,:,kk].T)).T\n",
    "mu, sigma = MOD.E_step(MOD.x,MOD.y, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "                      MOD.Q0, MOD.x0, MOD.R, X=S,\n",
    "                       poisson=poisson, disp=False)\n",
    "print('using noiseless y:\\n')\n",
    "print('neg lower bound given random C: ', MOD.L_obs(C0, MOD.B, ytmp, mu, sigma, X=S,Rtmp=MOD.R, poisson=poisson))\n",
    "print('neg lower bound given true C: ',MOD.L_obs(MOD.C, MOD.B, ytmp, mu, sigma, X=S,Rtmp=MOD.R, poisson=poisson))\n",
    "fig, ax = plt.subplots(1,2,figsize=(8,3))\n",
    "for xx in range(2):\n",
    "    ax[xx].plot(MOD.J_L_obs_C(C0, MOD.B, ytmp, mu, sigma, X=S,Rtmp=MOD.R, poisson=poisson)[:,xx], 'r', label='grad $C_{start}$')\n",
    "    ax[xx].plot(MOD.J_L_obs_C(MOD.C, MOD.B, ytmp, mu, sigma, X=S,Rtmp=MOD.R, poisson=poisson)[:,xx],'b', label='grad $C_{true}$')\n",
    "    ax[xx].set_title('deriv_C: gradient for dimension '+np.str(xx+1))\n",
    "    ax[xx].set_xlabel('neurons')\n",
    "    ax[xx].set_ylabel('gradient')\n",
    "ax[0].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytmp = np.zeros(MOD.y.shape)\n",
    "for kk in range(MOD.Ttrials):\n",
    "    if poisson:\n",
    "        ytmp[:,:,kk] = np.exp(MOD.C.dot(MOD.x[:,:,kk].T)+MOD.B.dot(S[:,:,kk].T)).T\n",
    "    else:\n",
    "        ytmp[:,:,kk] = (MOD.C.dot(MOD.x[:,:,kk].T)+MOD.B.dot(S[:,:,kk].T)).T\n",
    "mu, sigma = MOD.E_step(MOD.x,MOD.y, MOD.B, MOD.C, MOD.A, MOD.Q, \n",
    "                      MOD.Q0, MOD.x0, MOD.R, X=S,\n",
    "                       poisson=poisson, disp=False)\n",
    "print('using noiseless y:\\n')\n",
    "print('neg lower bound given random B: ', MOD.L_obs(MOD.C, B0, ytmp, mu, sigma, X=S, Rtmp=MOD.R, poisson=poisson))\n",
    "print('neg lower bound given true B: ',MOD.L_obs(MOD.C, MOD.B, ytmp, mu, sigma, X=S, Rtmp=MOD.R, poisson=poisson))\n",
    "fig, ax = plt.subplots(1,2,figsize=(8,3))\n",
    "for xx in range(S.shape[1]):\n",
    "    ax[xx].plot(MOD.J_L_obs_B(MOD.C, B0, ytmp, mu, sigma, X=S, Rtmp=MOD.R, poisson=poisson)[:,xx], 'r', label='grad $B_{start}$')\n",
    "    ax[xx].plot(MOD.J_L_obs_B(MOD.C, MOD.B, ytmp, mu, sigma, X=S, Rtmp=MOD.R, poisson=poisson)[:,xx],'b', label='grad $B_{true}$')\n",
    "    ax[xx].set_title('deriv_B: gradient for dimension '+np.str(xx+1))\n",
    "    ax[xx].set_xlabel('neurons')\n",
    "    ax[xx].set_ylabel('gradient')\n",
    "ax[0].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo\n",
    "- implement gaussian observation noise analytic and numeric update of C\n",
    "- use Hessian for C and B optimization?\n",
    "\n",
    "open questions\n",
    "- update of B: possible analytically or only via likelihood maximization\n",
    "- Q and Q0 always sytematically too small, why?\n",
    "\n",
    "note\n",
    "- had to tweak the optimization method in scipy minimize, to compute the latent posterior (E-step inference) Newton threw back error for the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
